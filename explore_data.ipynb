{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cee494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b48c1",
   "metadata": {},
   "source": [
    "- get data:\n",
    "    - `./download_data.sh --task all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06699845",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR='.cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0042b",
   "metadata": {},
   "source": [
    "### 1. parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032ef6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_langs=[\n",
    "    \"UD_Armenian-ArmTDP\", \"UD_Norwegian-Nynorsk\", \"UD_Portuguese-Bosque\", \"UD_Italian-PoSTWITA\", \"UD_Old_French-SRCMF\", \"UD_North_Sami-Giella\", \"UD_Norwegian-Bokmaal\", \"UD_French-ParisStories\", \"UD_Italian-MarkIT\", \"UD_Chinese-GSDSimp\", \"UD_English-EWT\", \"UD_French-Rhapsodie\", \"UD_French-ParTUT\", \"UD_Classical_Chinese-Kyoto\", \"UD_Norwegian-NynorskLIA\", \"UD_Arabic-NYUAD\", \"UD_Portuguese-PetroGold\", \"UD_Italian-TWITTIRO\", \"UD_Turkish_German-SAGT\", \"UD_Maghrebi_Arabic_French-Arabizi\", \"UD_Portuguese-CINTIL\", \"UD_Ligurian-GLT\", \"UD_Dutch-Alpino\", \"UD_Western_Armenian-ArmTDP\", \"UD_Portuguese-GSD\", \"singlish\", \"UD_Arabic-PADT\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b34856e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UD_Armenian-ArmTDP', 'UD_Norwegian-Nynorsk', 'UD_Portuguese-Bosque', 'UD_Italian-PoSTWITA', 'UD_Old_French-SRCMF', 'UD_North_Sami-Giella', 'UD_Norwegian-Bokmaal', 'UD_French-ParisStories', 'UD_Italian-MarkIT', 'UD_Chinese-GSDSimp', 'UD_English-EWT', 'UD_French-Rhapsodie', 'UD_French-ParTUT', 'UD_Classical_Chinese-Kyoto', 'UD_Norwegian-NynorskLIA', 'UD_Arabic-NYUAD', 'UD_Portuguese-PetroGold', 'UD_Italian-TWITTIRO', 'UD_Turkish_German-SAGT', 'UD_Maghrebi_Arabic_French-Arabizi', 'UD_Portuguese-CINTIL', 'UD_Ligurian-GLT', 'UD_Dutch-Alpino', 'UD_Western_Armenian-ArmTDP', 'UD_Portuguese-GSD', 'singlish', 'UD_Arabic-PADT']\n"
     ]
    }
   ],
   "source": [
    "print(train_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e95067",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('metadata/udp_metadata.json')\n",
    "metadata = json.load(f)\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0255012",
   "metadata": {},
   "source": [
    "- we finetune each lang where train split available\n",
    "- if train split not available, we perform zero-shot from UD_English-EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffb7764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>code</th>\n",
       "      <th>desc</th>\n",
       "      <th>langgroup</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>singlish</th>\n",
       "      <td>singlish</td>\n",
       "      <td>eng-sing</td>\n",
       "      <td></td>\n",
       "      <td>English</td>\n",
       "      <td>[dev, test, train]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Armenian-ArmTDP</th>\n",
       "      <td>Armenian</td>\n",
       "      <td>hye-east</td>\n",
       "      <td></td>\n",
       "      <td>Armenian</td>\n",
       "      <td>[train, test, dev]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_French-ParTUT</th>\n",
       "      <td>French</td>\n",
       "      <td>fre-multigenre</td>\n",
       "      <td></td>\n",
       "      <td>French</td>\n",
       "      <td>[train, dev, test]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_English-EWT</th>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td></td>\n",
       "      <td>English</td>\n",
       "      <td>[dev, test, train]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Ligurian-GLT</th>\n",
       "      <td>Ligurian</td>\n",
       "      <td>lij</td>\n",
       "      <td></td>\n",
       "      <td>Italian</td>\n",
       "      <td>[train, test]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Gheg-GPS</th>\n",
       "      <td>Gheg</td>\n",
       "      <td>aln</td>\n",
       "      <td></td>\n",
       "      <td>Albanian</td>\n",
       "      <td>[test]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Norwegian-Nynorsk</th>\n",
       "      <td>Norwegian</td>\n",
       "      <td>nor-nynorsk</td>\n",
       "      <td></td>\n",
       "      <td>Norwegian</td>\n",
       "      <td>[test, dev, train]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Albanian-TSA</th>\n",
       "      <td>Albanian</td>\n",
       "      <td>alb</td>\n",
       "      <td></td>\n",
       "      <td>Albanian</td>\n",
       "      <td>[test]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Italian-PUD</th>\n",
       "      <td>Italian</td>\n",
       "      <td>ita-trans</td>\n",
       "      <td></td>\n",
       "      <td>Italian</td>\n",
       "      <td>[test]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Portuguese-Bosque</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>por-euro-bra</td>\n",
       "      <td></td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>[test, dev, train]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            lang            code desc   langgroup  \\\n",
       "singlish                singlish        eng-sing          English   \n",
       "UD_Armenian-ArmTDP      Armenian        hye-east         Armenian   \n",
       "UD_French-ParTUT          French  fre-multigenre           French   \n",
       "UD_English-EWT           English             eng          English   \n",
       "UD_Ligurian-GLT         Ligurian             lij          Italian   \n",
       "UD_Gheg-GPS                 Gheg             aln         Albanian   \n",
       "UD_Norwegian-Nynorsk   Norwegian     nor-nynorsk        Norwegian   \n",
       "UD_Albanian-TSA         Albanian             alb         Albanian   \n",
       "UD_Italian-PUD           Italian       ita-trans          Italian   \n",
       "UD_Portuguese-Bosque  Portuguese    por-euro-bra       Portuguese   \n",
       "\n",
       "                                   split  \n",
       "singlish              [dev, test, train]  \n",
       "UD_Armenian-ArmTDP    [train, test, dev]  \n",
       "UD_French-ParTUT      [train, dev, test]  \n",
       "UD_English-EWT        [dev, test, train]  \n",
       "UD_Ligurian-GLT            [train, test]  \n",
       "UD_Gheg-GPS                       [test]  \n",
       "UD_Norwegian-Nynorsk  [test, dev, train]  \n",
       "UD_Albanian-TSA                   [test]  \n",
       "UD_Italian-PUD                    [test]  \n",
       "UD_Portuguese-Bosque  [test, dev, train]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lang=pd.DataFrame(metadata).T\n",
    "all_lang.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b24761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset universal_dependencies (.cache/universal_dependencies/UD_Armenian-ArmTDP/2.7.0/f5558234d16160340d9861389cbd2b7a134edb3ab5a8cb1173a1715c8b095b06)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 506.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#explore single lang\n",
    "lang='UD_Armenian-ArmTDP'\n",
    "dataset = load_dataset(\"scripts/universal_dependencies.py\", lang,\n",
    "            cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b483be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 1974\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 249\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 277\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86537b3a",
   "metadata": {},
   "source": [
    "- parsing use tokens and deprel column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e505170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Մտածում', 'եմ', '՝', 'Ադամի', 'ու', 'Եվայի', 'վտարումը', 'Եդեմական', 'այգուց', '(', 'դրախտից', ')', ',', 'նրանց', 'տեղափոխումն', 'այլ', 'վայր', ',', 'ուր', 'շրջակայքն', 'այլևս', 'բարեկամական', 'չէր', ',', 'այլ', 'խիստ', 'թշնամական', ',', 'ուր', 'իրենց', 'հացը', 'պիտի', 'տանջանքով', 'վաստակեին', ',', 'նույն', 'մոլորակի', 'սահմաններում', 'չէր', ',', 'որ', 'կատարվեց', ':'], ['Եդեմն', 'այլ', 'մոլորակ', 'էր', ',', 'աքսորավայրը', '՝', 'այլ', ',', 'այսինքն', '՝', 'այս', ',', 'ուր', 'այժմ', 'էլ', 'բնակվում', 'ենք', ',', 'բայց', 'միշտ', 'նայում', 'ենք', 'երկինք', '՝', 'բնազդում', 'դրոշմված', 'հիշողությամբ', 'Եդեմը', 'որոնելով', '։'], ['Իսկ', 'այն', 'չկա', ',', 'Տերը', 'պայթեցրել', 'է', 'կամ', 'գուցե', 'ամայացրել', ',', 'բնակության', 'համար', 'անպիտան', 'դարձրել', ',', 'կամ', 'էլ', 'կա', 'ու', 'ախտահանվում', 'է', '՝', 'նոր', 'բնակիչներ', 'ընդունելու', 'համար', '։'], ['Մի', 'խոսքով', '՝', 'մենք', 'դրա', 'հետ', 'էլ', 'գործ', 'չունենք', ',', 'մերը', 'չէ', 'այլևս', ',', 'մերը', 'սա', 'է', '՝', 'դժոխքը', ',', 'որը', ',', 'սակայն', ',', 'հասցրել', 'ենք', 'սիրել', '.', 'դեհ', ',', 'շանը', 'որտեղ', 'էլ', 'կապես', ',', 'կապվում', '-', 'ընտելանում', '-', 'սիրում', 'է', '։'], ['Իրականում', 'հեչ', 'սիրելու', 'բան', 'չի', '.', 'հա', ',', 'էլի', 'որ', '՝', 'կապույտ', 'երկինք', ',', 'ծովեր', ',', 'անտառներ', ',', 'դաշտեր', ':']]\n",
      "\n",
      "[['root', 'aux', 'punct', 'nmod:poss', 'cc', 'conj', 'nsubj', 'amod', 'nmod:npmod', 'punct', 'appos', 'punct', 'punct', 'det:poss', 'conj', 'det', 'nmod:npmod', 'punct', 'advmod', 'nsubj', 'advmod', 'acl:relcl', 'cop', 'punct', 'cc', 'advmod', 'conj', 'punct', 'advmod', 'det:poss', 'obj', 'aux', 'obl', 'conj', 'punct', 'det', 'nmod:poss', 'ccomp', 'cop', 'punct', 'mark', 'csubj', 'punct'], ['nsubj', 'det', 'root', 'cop', 'punct', 'conj', 'punct', 'orphan', 'punct', 'cc', 'punct', 'appos', 'punct', 'advmod', 'advmod', 'advmod', 'acl:relcl', 'aux', 'punct', 'cc', 'advmod', 'conj', 'aux', 'obl', 'punct', 'obl', 'acl', 'obl', 'obj', 'advcl', 'punct'], ['cc', 'nsubj', 'root', 'punct', 'nsubj', 'parataxis', 'aux', 'cc', 'discourse', 'conj', 'punct', 'obl', 'case', 'xcomp', 'conj', 'punct', 'cc', 'fixed', 'conj', 'cc', 'conj', 'aux', 'punct', 'amod', 'obj', 'obl', 'case', 'punct'], ['nummod', 'parataxis', 'punct', 'nsubj', 'obl', 'case', 'advmod:emph', 'compound:lvc', 'root', 'punct', 'conj', 'cop', 'advmod:emph', 'punct', 'nsubj', 'conj', 'cop', 'punct', 'appos', 'punct', 'nsubj', 'punct', 'discourse', 'punct', 'acl:relcl', 'aux', 'xcomp', 'punct', 'discourse', 'punct', 'obj', 'advmod', 'advmod', 'parataxis', 'punct', 'conj', 'punct', 'conj', 'punct', 'conj', 'aux', 'punct'], ['advmod', 'advmod', 'nmod:poss', 'root', 'cop', 'punct', 'discourse', 'punct', 'discourse', 'fixed', 'punct', 'amod', 'parataxis', 'punct', 'conj', 'punct', 'conj', 'punct', 'conj', 'punct']]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['tokens'][:5])\n",
    "print()\n",
    "print(dataset['train']['deprel'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90899dbf",
   "metadata": {},
   "source": [
    "### 2. pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d3a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_langs=[\"UD_Armenian-ArmTDP\", \"UD_Norwegian-Nynorsk\", \"UD_Portuguese-Bosque\", \"UD_Italian-PoSTWITA\", \"UD_Old_French-SRCMF\", \"UD_North_Sami-Giella\", \"UD_Norwegian-Bokmaal\", \"UD_French-ParisStories\", \"UD_Italian-MarkIT\", \"UD_Chinese-GSDSimp\", \"UD_English-EWT\", \"UD_French-Rhapsodie\", \"UD_French-ParTUT\", \"UD_Classical_Chinese-Kyoto\", \"UD_Norwegian-NynorskLIA\", \"UD_Arabic-NYUAD\", \"UD_Portuguese-PetroGold\", \"UD_Italian-TWITTIRO\", \"UD_Turkish_German-SAGT\", \"UD_Maghrebi_Arabic_French-Arabizi\", \"UD_Portuguese-CINTIL\", \"UD_Ligurian-GLT\", \"UD_Dutch-Alpino\", \"UD_Western_Armenian-ArmTDP\", \"UD_Portuguese-GSD\", \"singlish\", \"UD_Arabic-PADT\", \"UD_French-GSD\", \"UD_Catalan-AnCora\", \"UD_Estonian-EDT\", \"UD_Finnish-TDT\", \"UD_Spanish-AnCora\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e16bda87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UD_Armenian-ArmTDP', 'UD_Norwegian-Nynorsk', 'UD_Portuguese-Bosque', 'UD_Italian-PoSTWITA', 'UD_Old_French-SRCMF', 'UD_North_Sami-Giella', 'UD_Norwegian-Bokmaal', 'UD_French-ParisStories', 'UD_Italian-MarkIT', 'UD_Chinese-GSDSimp', 'UD_English-EWT', 'UD_French-Rhapsodie', 'UD_French-ParTUT', 'UD_Classical_Chinese-Kyoto', 'UD_Norwegian-NynorskLIA', 'UD_Arabic-NYUAD', 'UD_Portuguese-PetroGold', 'UD_Italian-TWITTIRO', 'UD_Turkish_German-SAGT', 'UD_Maghrebi_Arabic_French-Arabizi', 'UD_Portuguese-CINTIL', 'UD_Ligurian-GLT', 'UD_Dutch-Alpino', 'UD_Western_Armenian-ArmTDP', 'UD_Portuguese-GSD', 'singlish', 'UD_Arabic-PADT', 'UD_French-GSD', 'UD_Catalan-AnCora', 'UD_Estonian-EDT', 'UD_Finnish-TDT', 'UD_Spanish-AnCora']\n"
     ]
    }
   ],
   "source": [
    "print(train_langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97375a95",
   "metadata": {},
   "source": [
    "- we finetune each lang where train split available\n",
    "- if train split not available, we perform zero-shot from UD_English-EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40029288",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('metadata/pos_metadata.json')\n",
    "metadata = json.load(f)\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18ddb29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>code</th>\n",
       "      <th>desc</th>\n",
       "      <th>langgroup</th>\n",
       "      <th>split</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>singlish</th>\n",
       "      <td>singlish</td>\n",
       "      <td>eng-sing</td>\n",
       "      <td></td>\n",
       "      <td>English</td>\n",
       "      <td>[dev, test, train]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROci</th>\n",
       "      <td>occitan</td>\n",
       "      <td>ROci</td>\n",
       "      <td></td>\n",
       "      <td>French-occ</td>\n",
       "      <td>[test]</td>\n",
       "      <td>noisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Armenian-ArmTDP</th>\n",
       "      <td>Armenian</td>\n",
       "      <td>hye-east</td>\n",
       "      <td></td>\n",
       "      <td>Armenian</td>\n",
       "      <td>[train, test, dev]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_French-ParTUT</th>\n",
       "      <td>French</td>\n",
       "      <td>fre-multigenre</td>\n",
       "      <td></td>\n",
       "      <td>French</td>\n",
       "      <td>[train, dev, test]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_English-EWT</th>\n",
       "      <td>English</td>\n",
       "      <td>eng</td>\n",
       "      <td></td>\n",
       "      <td>English</td>\n",
       "      <td>[dev, test, train]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Ligurian-GLT</th>\n",
       "      <td>Ligurian</td>\n",
       "      <td>lij</td>\n",
       "      <td></td>\n",
       "      <td>Italian</td>\n",
       "      <td>[train, test]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Gheg-GPS</th>\n",
       "      <td>Gheg</td>\n",
       "      <td>aln</td>\n",
       "      <td></td>\n",
       "      <td>Albanian</td>\n",
       "      <td>[test]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Norwegian-Nynorsk</th>\n",
       "      <td>Norwegian</td>\n",
       "      <td>nor-nynorsk</td>\n",
       "      <td></td>\n",
       "      <td>Norwegian</td>\n",
       "      <td>[test, dev, train]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Albanian-TSA</th>\n",
       "      <td>Albanian</td>\n",
       "      <td>alb</td>\n",
       "      <td></td>\n",
       "      <td>Albanian</td>\n",
       "      <td>[test]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD_Italian-PUD</th>\n",
       "      <td>Italian</td>\n",
       "      <td>ita-trans</td>\n",
       "      <td></td>\n",
       "      <td>Italian</td>\n",
       "      <td>[test]</td>\n",
       "      <td>ud</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           lang            code desc   langgroup  \\\n",
       "singlish               singlish        eng-sing          English   \n",
       "ROci                    occitan            ROci       French-occ   \n",
       "UD_Armenian-ArmTDP     Armenian        hye-east         Armenian   \n",
       "UD_French-ParTUT         French  fre-multigenre           French   \n",
       "UD_English-EWT          English             eng          English   \n",
       "UD_Ligurian-GLT        Ligurian             lij          Italian   \n",
       "UD_Gheg-GPS                Gheg             aln         Albanian   \n",
       "UD_Norwegian-Nynorsk  Norwegian     nor-nynorsk        Norwegian   \n",
       "UD_Albanian-TSA        Albanian             alb         Albanian   \n",
       "UD_Italian-PUD          Italian       ita-trans          Italian   \n",
       "\n",
       "                                   split dataset  \n",
       "singlish              [dev, test, train]      ud  \n",
       "ROci                              [test]   noisy  \n",
       "UD_Armenian-ArmTDP    [train, test, dev]      ud  \n",
       "UD_French-ParTUT      [train, dev, test]      ud  \n",
       "UD_English-EWT        [dev, test, train]      ud  \n",
       "UD_Ligurian-GLT            [train, test]      ud  \n",
       "UD_Gheg-GPS                       [test]      ud  \n",
       "UD_Norwegian-Nynorsk  [test, dev, train]      ud  \n",
       "UD_Albanian-TSA                   [test]      ud  \n",
       "UD_Italian-PUD                    [test]      ud  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lang=pd.DataFrame(metadata).T\n",
    "all_lang.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72581780",
   "metadata": {},
   "source": [
    "#### use different dataset loading script depending on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5260ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset universal_dependencies (.cache/universal_dependencies/singlish/2.7.0/f5558234d16160340d9861389cbd2b7a134edb3ab5a8cb1173a1715c8b095b06)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 565.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 2465\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 286\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
      "        num_rows: 299\n",
      "    })\n",
      "})\n",
      "[['bt', 'still', 'okie', 'la', 'if', 'go', 'hong', 'kong', '.', '.'], ['Semb', 'Corp', '3.28', 'coming', '.']]\n",
      "[[10, 14, 6, 15, 5, 16, 10, 10, 1, 1], [10, 10, 3, 16, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang='singlish' ## metadata[lang]['dataset']=='ud':\n",
    "script=\"scripts/universal_dependencies.py\"\n",
    "predict_dataset = load_dataset(script, lang, cache_dir=CACHE_DIR)\n",
    "print(predict_dataset)\n",
    "print(predict_dataset['test']['tokens'][:2])\n",
    "print(predict_dataset['test']['upos'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a0cffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ROci-data_dir=data%2Fpos_tagging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset noisy_dialect/ROci to .cache/noisy_dialect/ROci-data_dir=data%2Fpos_tagging/1.1.0/a0e4190d7e72716271d4c2496d5dcf596262a4082468db5160758db6f295efe5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset noisy_dialect downloaded and prepared to .cache/noisy_dialect/ROci-data_dir=data%2Fpos_tagging/1.1.0/a0e4190d7e72716271d4c2496d5dcf596262a4082468db5160758db6f295efe5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 790.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'upos'],\n",
      "        num_rows: 874\n",
      "    })\n",
      "})\n",
      "[['Puei', ',', 'sabètz', ',', 'se', 'fins_a', 'cinc', 'ans', 'avètz', 'gaire', 'de', 'sovenirs', ',', 'après', 'un', 'pichòt', 'saup', 'chifrar', 'e', 'pòu', 'jutjar', 'de', 'son', 'sicut', '.'], ['Me', 'rèstan', 'pasmens', 'quauquei', 'sovenirs', \"d'\", 'avans', 'cinc', 'ans', '.']]\n",
      "[[14, 1, 16, 1, 5, 2, 3, 0, 16, 14, 2, 0, 1, 2, 8, 0, 16, 16, 9, 16, 16, 2, 8, 0, 1], [11, 16, 14, 11, 0, 2, 2, 3, 0, 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang='ROci' ## metadata[lang]['dataset']=='ud':\n",
    "script=\"scripts/pos_tagging/noisy_dialect.py\"\n",
    "data_dir=\"data/pos_tagging\"\n",
    "predict_dataset = load_dataset(script, lang, data_dir=data_dir,cache_dir=CACHE_DIR)\n",
    "print(predict_dataset)\n",
    "print(predict_dataset['test']['tokens'][:2])\n",
    "print(predict_dataset['test']['upos'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043579f",
   "metadata": {},
   "source": [
    "### 3. ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b300c9",
   "metadata": {},
   "source": [
    "- we train one language/dialect per language/dialect group and evaluate on the whole group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "868e8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiann_train_langs=[\"ar\", \"az\", \"ku\", \"tr\", \"hsb\", \"nl\", \"fr\", \"zh\", \"en\", \"mhr\", \"it\", \"de\", \"pa\", \"es\", \"hr\", \"lv\", \"hi\", \"ro\", \"el\", \"bn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2988a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'az', 'ku', 'tr', 'hsb', 'nl', 'fr', 'zh', 'en', 'mhr', 'it', 'de', 'pa', 'es', 'hr', 'lv', 'hi', 'ro', 'el', 'bn']\n"
     ]
    }
   ],
   "source": [
    "print(wikiann_train_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b18ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "norwegian_train_langs=[\"bokmaal\" ,\"nynorsk\" ,\"samnorsk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45b03120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bokmaal', 'nynorsk', 'samnorsk']\n"
     ]
    }
   ],
   "source": [
    "print(norwegian_train_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b93e4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('metadata/ner_metadata.json')\n",
    "metadata = json.load(f)\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf280b0",
   "metadata": {},
   "source": [
    "- langgroup corresponds to the training language per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "039ad1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>code</th>\n",
       "      <th>langgroup</th>\n",
       "      <th>huggingface</th>\n",
       "      <th>dataset</th>\n",
       "      <th>region</th>\n",
       "      <th>train_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>english</td>\n",
       "      <td>en</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>english</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>arabic</td>\n",
       "      <td>ar</td>\n",
       "      <td>arabic</td>\n",
       "      <td>True</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>arabic</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arz</th>\n",
       "      <td>egyptian arabic</td>\n",
       "      <td>arz</td>\n",
       "      <td>arabic</td>\n",
       "      <td>True</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>arabic</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kab</th>\n",
       "      <td>kabyle</td>\n",
       "      <td>kab</td>\n",
       "      <td>arabic</td>\n",
       "      <td>False</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>arabic</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbd</th>\n",
       "      <td>kabardian</td>\n",
       "      <td>kbd</td>\n",
       "      <td>adyghe</td>\n",
       "      <td>False</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>adyghe</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ady</th>\n",
       "      <td>adyghe</td>\n",
       "      <td>ady</td>\n",
       "      <td>adyghe</td>\n",
       "      <td>False</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>adyghe</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>az</th>\n",
       "      <td>azerbaijani</td>\n",
       "      <td>az</td>\n",
       "      <td>azerbaijani</td>\n",
       "      <td>True</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>azerbaijani</td>\n",
       "      <td>az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>azb</th>\n",
       "      <td>south azerbaijani</td>\n",
       "      <td>azb</td>\n",
       "      <td>azerbaijani</td>\n",
       "      <td>False</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>azerbaijani</td>\n",
       "      <td>az</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ckb</th>\n",
       "      <td>central kurdish</td>\n",
       "      <td>ckb</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>True</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>ku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ku</th>\n",
       "      <td>kurdish</td>\n",
       "      <td>ku</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>True</td>\n",
       "      <td>wikiann</td>\n",
       "      <td>kurdish</td>\n",
       "      <td>ku</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  lang code    langgroup huggingface  dataset       region  \\\n",
       "en             english   en      english        True  wikiann      english   \n",
       "ar              arabic   ar       arabic        True  wikiann       arabic   \n",
       "arz    egyptian arabic  arz       arabic        True  wikiann       arabic   \n",
       "kab             kabyle  kab       arabic       False  wikiann       arabic   \n",
       "kbd          kabardian  kbd       adyghe       False  wikiann       adyghe   \n",
       "ady             adyghe  ady       adyghe       False  wikiann       adyghe   \n",
       "az         azerbaijani   az  azerbaijani        True  wikiann  azerbaijani   \n",
       "azb  south azerbaijani  azb  azerbaijani       False  wikiann  azerbaijani   \n",
       "ckb    central kurdish  ckb      kurdish        True  wikiann      kurdish   \n",
       "ku             kurdish   ku      kurdish        True  wikiann      kurdish   \n",
       "\n",
       "    train_lang  \n",
       "en          en  \n",
       "ar          ar  \n",
       "arz         ar  \n",
       "kab         ar  \n",
       "kbd         en  \n",
       "ady         en  \n",
       "az          az  \n",
       "azb         az  \n",
       "ckb         ku  \n",
       "ku          ku  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lang=pd.DataFrame(metadata).T\n",
    "all_lang.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f3d3b",
   "metadata": {},
   "source": [
    "- different dataset loading script for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e827dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikiann (.cache/wikiann/ar/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 988.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n",
      "[['تعلم', 'في', 'جامعة', 'نورث', 'وسترن', 'في', '.'], ['تحويل', 'ده\\u200cشهر', '(', 'مقاطعة', 'كلاردشت', ')']]\n",
      "[[0, 0, 3, 4, 4, 4, 0], [0, 5, 6, 6, 6, 6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## for langs not present in huggingface wikiann [huggingface=True]\n",
    "lang='ar' \n",
    "script=\"wikiann\"\n",
    "predict_dataset = load_dataset(script, lang, cache_dir=CACHE_DIR)\n",
    "print(predict_dataset)\n",
    "print(predict_dataset['test']['tokens'][:2])\n",
    "print(predict_dataset['test']['ner_tags'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6466b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikiann_og/kab to .cache/wikiann_og/kab/1.1.0/bd5069ae42633af8e332aa82fb4f866eafd1dffc876e6bae94c18978db74e5d7...\n",
      "<datasets.download.download_manager.ArchiveIterable object at 0x7fa042433d90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikiann_og downloaded and prepared to .cache/wikiann_og/kab/1.1.0/bd5069ae42633af8e332aa82fb4f866eafd1dffc876e6bae94c18978db74e5d7. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 792.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 3004\n",
      "    })\n",
      "})\n",
      "[['Aṣqif', 'n', 'Ṭmana'], ['Tizi', 'Wezzu']]\n",
      "[[5, 6, 6], [5, 6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## for test langs not present in huggingface wikiann [huggingface=False]\n",
    "lang='kab' \n",
    "script=\"scripts/ner/wikiann_og.py\"\n",
    "predict_dataset = load_dataset(script, lang, cache_dir=CACHE_DIR)\n",
    "print(predict_dataset)\n",
    "print(predict_dataset['test']['tokens'][:2])\n",
    "print(predict_dataset['test']['ner_tags'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7feaf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset norwegian_ner/bokmaal to .cache/norwegian_ner/bokmaal/1.0.0/d326c5669fdd52e99c78eaa6c9c8a8758a48f7494bf59ed17e12814eb349e443...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1908.24it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1174.22it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset norwegian_ner downloaded and prepared to .cache/norwegian_ner/bokmaal/1.0.0/d326c5669fdd52e99c78eaa6c9c8a8758a48f7494bf59ed17e12814eb349e443. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 516.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'pos_tags', 'ner_tags'],\n",
      "        num_rows: 15696\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'pos_tags', 'ner_tags'],\n",
      "        num_rows: 2410\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'text', 'tokens', 'lemmas', 'pos_tags', 'ner_tags'],\n",
      "        num_rows: 1939\n",
      "    })\n",
      "})\n",
      "[['Honnørordene', 'er', '\"', 'dristig', 'formspråk', '\"', ',', '\"', 'nyskapning', '\"', 'og', '\"', 'livgivende', 'kontrast', '\"', '.'], ['Jeg', 'ser', 'et', 'landskap', 'som', 'er', 'såret', 'og', 'i', 'tilbaketrekning', '.']]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## for NORWEGIAN_NER all_lang[all_lang['dataset']=='norwegian_ner']\n",
    "lang='bokmaal' \n",
    "script=\"scripts/ner/norwegian_ner.py\"\n",
    "predict_dataset = load_dataset(script, lang, cache_dir=CACHE_DIR)\n",
    "print(predict_dataset)\n",
    "print(predict_dataset['test']['tokens'][:2])\n",
    "print(predict_dataset['test']['ner_tags'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be9700b",
   "metadata": {},
   "source": [
    "### 4. dialect identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6483a46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic\n",
      "test.csv\n",
      "{'SAN': 200, 'JER': 200, 'BEI': 200, 'SAL': 200, 'AMM': 200, 'BAG': 200, 'RIY': 200, 'ALE': 200, 'MUS': 200, 'SFX': 200, 'ALG': 200, 'BAS': 200, 'JED': 200, 'TRI': 200, 'MOS': 200, 'ALX': 200, 'FES': 200, 'CAI': 200, 'TUN': 200, 'RAB': 200, 'MSA': 200, 'KHA': 200, 'DOH': 200, 'ASW': 200, 'DAM': 200, 'BEN': 200}\n",
      "\n",
      "dev.csv\n",
      "{'ASW': 200, 'BAS': 200, 'JER': 200, 'AMM': 200, 'TUN': 200, 'JED': 200, 'RAB': 200, 'SFX': 200, 'MSA': 200, 'DAM': 200, 'ALG': 200, 'BEI': 200, 'TRI': 200, 'KHA': 200, 'SAL': 200, 'RIY': 200, 'ALE': 200, 'MUS': 200, 'BEN': 200, 'MOS': 200, 'BAG': 200, 'FES': 200, 'CAI': 200, 'DOH': 200, 'SAN': 200, 'ALX': 200}\n",
      "\n",
      "train.csv\n",
      "{'KHA': 1600, 'RAB': 1600, 'DOH': 1600, 'DAM': 1600, 'ALX': 1600, 'AMM': 1600, 'SAL': 1600, 'JER': 1600, 'TUN': 1600, 'MUS': 1600, 'BAS': 1600, 'FES': 1600, 'TRI': 1600, 'ASW': 1600, 'MSA': 1600, 'BEI': 1600, 'RIY': 1600, 'BAG': 1600, 'BEN': 1600, 'SFX': 1600, 'ALE': 1600, 'MOS': 1600, 'CAI': 1600, 'JED': 1600, 'ALG': 1600, 'SAN': 1600}\n",
      "\n",
      "mandarin_traditional\n",
      "dev.csv\n",
      "{'M': 1000, 'T': 1000}\n",
      "\n",
      "train.csv\n",
      "{'M': 1000, 'T': 1000}\n",
      "\n",
      "\n",
      "swiss-dialects\n",
      "test.csv\n",
      "{'BE': 380, 'BS': 351, 'LU': 347, 'ZH': 345}\n",
      "\n",
      "train.csv\n",
      "{'BS': 848, 'ZH': 832, 'LU': 829, 'BE': 811}\n",
      "\n",
      "\n",
      "english\n",
      "dev.csv\n",
      "{'EN-US': 312, 'EN-GB': 211, 'EN': 76}\n",
      "\n",
      "train.csv\n",
      "{'EN-US': 312, 'EN-GB': 211, 'EN': 76}\n",
      "\n",
      "\n",
      "greek\n",
      "test.csv\n",
      "{'smg_twitter': 59, 'cg_other': 58, 'cg_twitter': 45, 'cg_fb': 30, 'smg_other': 23, 'smg_fb': 12}\n",
      "\n",
      "train.csv\n",
      "{'smg_twitter': 146, 'cg_other': 137, 'cg_twitter': 108, 'cg_fb': 54, 'smg_other': 52, 'smg_fb': 30}\n",
      "\n",
      "\n",
      "portuguese\n",
      "portuguese-dev.csv\n",
      "{'PT-BR': 588, 'PT-PT': 269, 'PT': 134}\n",
      "\n",
      "dev.csv\n",
      "{'PT-BR': 588, 'PT-PT': 269, 'PT': 134}\n",
      "\n",
      "train.csv\n",
      "{'PT-BR': 588, 'PT-PT': 269, 'PT': 134}\n",
      "\n",
      "portuguese-train.csv\n",
      "{'PT-BR': 588, 'PT-PT': 269, 'PT': 134}\n",
      "\n",
      "\n",
      "mandarin_simplified\n",
      "dev.csv\n",
      "{'M': 1000, 'T': 1000}\n",
      "\n",
      "train.csv\n",
      "{'M': 1000, 'T': 1000}\n",
      "\n",
      "\n",
      "spanish\n",
      "dev.csv\n",
      "{'ES-ES': 444, 'ES': 318, 'ES-AR': 227}\n",
      "\n",
      "train.csv\n",
      "{'ES-ES': 444, 'ES': 318, 'ES-AR': 227}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arabic='data/dialect-identification/arabic/MADAR/MADAR_Corpus'\n",
    "print('arabic')\n",
    "for f in os.listdir(arabic):\n",
    "    print(f)\n",
    "    df=pd.read_csv(os.path.join(arabic,f))\n",
    "    print(dict(df['label'].value_counts()))\n",
    "    print()\n",
    "    \n",
    "root='data/dialect-identification/'\n",
    "for f in os.listdir(root):\n",
    "    if f!='arabic' and not str(f).startswith('.'):\n",
    "        print(f)\n",
    "        for f1 in os.listdir(os.path.join(root,f)):\n",
    "            print(f1)\n",
    "            df=pd.read_csv(os.path.join(root,f,f1))\n",
    "            print(dict(df['label'].value_counts()))\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0484204",
   "metadata": {},
   "source": [
    "### 5. question-anwering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02dfe75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datapath='data/Question-Answering/SDQA-gold-task/sdqa-train-all.json'\n",
    "dev_datapath='data/Question-Answering/SDQA-gold-task/sdqa-dev-all.json'\n",
    "test_datapath='data/Question-Answering/SDQA-gold-task/sdqa-test-all.json'\n",
    "language_dialect_identifier=\"{lang}-id-{dialect}\" #look at id field, eg: \"english-6037841464917965779-nga\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6f5cd",
   "metadata": {},
   "source": [
    "### 6. sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "796dbded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic\n",
      "aeb_Latn-train.csv\n",
      "{'positive': 36942, 'negative': 30589, 'neutral': 2468}\n",
      "\n",
      "arb_arab-train.csv\n",
      "{'positive': 4797, 'neutral': 1923, 'negative': 1783, 'mixed': 324}\n",
      "\n",
      "arq_arab-test.csv\n",
      "{'negative': 7512, 'positive': 7448}\n",
      "\n",
      "ar-lb_arab-test.csv\n",
      "{5: 665, 4: 232, 3: 142, 1: 99, 2: 37}\n",
      "\n",
      "aeb_Arab-test.csv\n",
      "{'positive': 1701, 'negative': 1701}\n",
      "\n",
      "sau_arab-train.csv\n",
      "{'negative': 6080, 'positive': 1487, 'neutral': 866}\n",
      "\n",
      "jor_arab-test.csv\n",
      "{'positive': 295, 'negative': 245}\n",
      "\n",
      "ary_arab-test.csv\n",
      "{'positive': 914, 'neutral': 389, 'negative': 279, 'mixed': 58}\n",
      "\n",
      "jor_arab-train.csv\n",
      "{'negative': 655, 'positive': 605}\n",
      "\n",
      "aeb_Latn-test.csv\n",
      "{'positive': 15769, 'negative': 13178, 'neutral': 1054}\n",
      "\n",
      "ar-lb_arab-train.csv\n",
      "{5: 1648, 4: 502, 3: 276, 1: 204, 2: 111}\n",
      "\n",
      "arz_arab-test.csv\n",
      "{'objective': 2028, 'negative': 486, 'neutral': 254, 'positive': 235}\n",
      "\n",
      "arz_arab-train.csv\n",
      "{'objective': 4663, 'negative': 1198, 'neutral': 578, 'positive': 564}\n",
      "\n",
      "aeb_Arab-train.csv\n",
      "{'positive': 7155, 'negative': 6516}\n",
      "\n",
      "arb_arab-test.csv\n",
      "{'positive': 2103, 'neutral': 805, 'negative': 740, 'mixed': 136}\n",
      "\n",
      "ary_arab-train.csv\n",
      "{'positive': 2076, 'neutral': 920, 'negative': 703, 'mixed': 125}\n",
      "\n",
      "sau_arab-test.csv\n",
      "{'negative': 2589, 'positive': 656, 'neutral': 370}\n",
      "\n",
      "arq_arab-train.csv\n",
      "{'positive': 17484, 'negative': 17420}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arabic='data/sentiment_analysis/arabic'\n",
    "print('arabic')\n",
    "for f in os.listdir(arabic):\n",
    "    print(f)\n",
    "    df=pd.read_csv(os.path.join(arabic,f))\n",
    "    print(dict(df['label'].value_counts()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca54e3",
   "metadata": {},
   "source": [
    "### 7. topic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955c8aa",
   "metadata": {},
   "source": [
    "- we train one lang per group (identifier: langgroup) and evaluate on all other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877573c",
   "metadata": {},
   "source": [
    "#### we only consider the following languages from the sib-200 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a5e45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('metadata/topic_metadata.json')\n",
    "metadata = json.load(f)\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "221bd3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>code</th>\n",
       "      <th>langgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lmo_Latn</th>\n",
       "      <td>lombard</td>\n",
       "      <td>lmo_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng_Latn</th>\n",
       "      <td>English</td>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_Latn</th>\n",
       "      <td>italian</td>\n",
       "      <td>ita_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fur_Latn</th>\n",
       "      <td>friulian</td>\n",
       "      <td>fur_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scn_Latn</th>\n",
       "      <td>sicilian</td>\n",
       "      <td>scn_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srd_Latn</th>\n",
       "      <td>sardinian</td>\n",
       "      <td>srd_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vec_Latn</th>\n",
       "      <td>venetian</td>\n",
       "      <td>vec_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>azb_Arab</th>\n",
       "      <td>south</td>\n",
       "      <td>azb_Arab</td>\n",
       "      <td>azarbaijani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>azj_Latn</th>\n",
       "      <td>north</td>\n",
       "      <td>azj_Latn</td>\n",
       "      <td>azarbaijani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tur_Latn</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>tur_Latn</td>\n",
       "      <td>azarbaijani</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lang      code    langgroup\n",
       "lmo_Latn    lombard  lmo_Latn      italian\n",
       "eng_Latn    English  eng_Latn      English\n",
       "ita_Latn    italian  ita_Latn      italian\n",
       "fur_Latn   friulian  fur_Latn      italian\n",
       "scn_Latn   sicilian  scn_Latn      italian\n",
       "srd_Latn  sardinian  srd_Latn      italian\n",
       "vec_Latn   venetian  vec_Latn      italian\n",
       "azb_Arab      south  azb_Arab  azarbaijani\n",
       "azj_Latn      north  azj_Latn  azarbaijani\n",
       "tur_Latn    Turkish  tur_Latn  azarbaijani"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lang=pd.DataFrame(metadata).T\n",
    "all_lang.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ef002f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n",
      "{'science/technology': 51, 'travel': 40, 'politics': 30, 'sports': 25, 'health': 22, 'entertainment': 19, 'geography': 17}\n",
      "\n",
      "dev.csv\n",
      "{'science/technology': 25, 'travel': 20, 'politics': 14, 'sports': 12, 'health': 11, 'entertainment': 9, 'geography': 8}\n",
      "\n",
      "train.csv\n",
      "{'science/technology': 176, 'travel': 138, 'politics': 102, 'sports': 85, 'health': 77, 'entertainment': 65, 'geography': 58}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datapath='data/topic_class'\n",
    "lang='lmo_Latn'\n",
    "\n",
    "for f in os.listdir(os.path.join(datapath, lang)):\n",
    "    if f!='labels.txt':\n",
    "        print(f)\n",
    "        df=pd.read_csv(os.path.join(datapath,lang,f))\n",
    "        print(dict(df['label'].value_counts()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90834de",
   "metadata": {},
   "source": [
    "### 8. reading comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5342954",
   "metadata": {},
   "source": [
    "- we only consider the following evaluation languages from Belebele dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0fc9bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('metadata/rcmc_metadata.json')\n",
    "metadata = json.load(f)\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29e95cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>code</th>\n",
       "      <th>langgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zho_Hans</th>\n",
       "      <td>Chinese (Simplified)</td>\n",
       "      <td>zho_Hans</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zho_Hant</th>\n",
       "      <td>Chinese (Traditional)</td>\n",
       "      <td>zho_Hant</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng_Latn</th>\n",
       "      <td>English</td>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nso_Latn</th>\n",
       "      <td>Northern Sotho</td>\n",
       "      <td>nso_Latn</td>\n",
       "      <td>sotho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sot_Latn</th>\n",
       "      <td>Southern Sotho</td>\n",
       "      <td>sot_Latn</td>\n",
       "      <td>sotho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acm_Arab</th>\n",
       "      <td>Mesopotamian Arabic</td>\n",
       "      <td>acm_Arab</td>\n",
       "      <td>arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apc_Arab</th>\n",
       "      <td>North Levantine Arabic</td>\n",
       "      <td>apc_Arab</td>\n",
       "      <td>arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arb_Arab</th>\n",
       "      <td>MSA (Arabic)</td>\n",
       "      <td>arb_Arab</td>\n",
       "      <td>arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ars_Arab</th>\n",
       "      <td>Najdi Arabic</td>\n",
       "      <td>ars_Arab</td>\n",
       "      <td>arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ary_Arab</th>\n",
       "      <td>Moroccan Arabic</td>\n",
       "      <td>ary_Arab</td>\n",
       "      <td>arabic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            lang      code langgroup\n",
       "zho_Hans    Chinese (Simplified)  zho_Hans   chinese\n",
       "zho_Hant   Chinese (Traditional)  zho_Hant   chinese\n",
       "eng_Latn                 English  eng_Latn   English\n",
       "nso_Latn          Northern Sotho  nso_Latn     sotho\n",
       "sot_Latn          Southern Sotho  sot_Latn     sotho\n",
       "acm_Arab     Mesopotamian Arabic  acm_Arab    arabic\n",
       "apc_Arab  North Levantine Arabic  apc_Arab    arabic\n",
       "arb_Arab            MSA (Arabic)  arb_Arab    arabic\n",
       "ars_Arab            Najdi Arabic  ars_Arab    arabic\n",
       "ary_Arab         Moroccan Arabic  ary_Arab    arabic"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lang=pd.DataFrame(metadata).T\n",
    "all_lang.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39385c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': 'https:\\\\/\\\\/en.wikibooks.org\\\\/wiki\\\\/Accordion\\\\/Right_hand', 'question_number': 1, 'flores_passage': \"Make sure your hand is as relaxed as possible while still hitting all the notes correctly - also try not to make much extraneous motion with your fingers. This way, you will tire yourself out as little as possible. Remember there's no need to hit the keys with a lot of force for extra volume like on the piano. On the accordion, to get extra volume, you use the bellows with more pressure or speed.\", 'question': 'According to the passage, what would not be considered an accurate tip for successfully playing the accordion?', 'mc_answer1': 'For additional volume, increase the force with which you hit the keys', 'mc_answer2': 'Keep unnecessary movement to a minimum in order to preserve your stamina', 'mc_answer3': 'Be mindful of hitting the notes while maintaining a relaxed hand', 'mc_answer4': 'Increase the speed with which you operate the bellows to achieve extra volume', 'correct_answer_num': '1', 'dialect': 'eng_Latn', 'ds': '2023-05-03'}\n"
     ]
    }
   ],
   "source": [
    "datapath='datapath/reading-comprehension/Belebele'\n",
    "sample_test_file='arb_Arab.jsonl'\n",
    "first_example={\"link\":\"https:\\/\\/en.wikibooks.org\\/wiki\\/Accordion\\/Right_hand\",\"question_number\":1,\"flores_passage\":\"Make sure your hand is as relaxed as possible while still hitting all the notes correctly - also try not to make much extraneous motion with your fingers. This way, you will tire yourself out as little as possible. Remember there's no need to hit the keys with a lot of force for extra volume like on the piano. On the accordion, to get extra volume, you use the bellows with more pressure or speed.\",\"question\":\"According to the passage, what would not be considered an accurate tip for successfully playing the accordion?\",\"mc_answer1\":\"For additional volume, increase the force with which you hit the keys\",\"mc_answer2\":\"Keep unnecessary movement to a minimum in order to preserve your stamina\",\"mc_answer3\":\"Be mindful of hitting the notes while maintaining a relaxed hand\",\"mc_answer4\":\"Increase the speed with which you operate the bellows to achieve extra volume\",\"correct_answer_num\":\"1\",\"dialect\":\"eng_Latn\",\"ds\":\"2023-05-03\"}\n",
    "print(first_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84eb6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_file=datapath='datapath/reading-comprehension/Belebele/train.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918c5d2",
   "metadata": {},
   "source": [
    "### 9. Natural language Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe94a6a",
   "metadata": {},
   "source": [
    "- we perform cross-lingual transfer on translate-test langs from eng_Latn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df251e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('metadata/nli_metadata.json')\n",
    "metadata = json.load(f)\n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57bffc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>code</th>\n",
       "      <th>langgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lmo_Latn</th>\n",
       "      <td>lombard</td>\n",
       "      <td>lmo_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng_Latn</th>\n",
       "      <td>English</td>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita_Latn</th>\n",
       "      <td>italian</td>\n",
       "      <td>ita_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fur_Latn</th>\n",
       "      <td>friulian</td>\n",
       "      <td>fur_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scn_Latn</th>\n",
       "      <td>sicilian</td>\n",
       "      <td>scn_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>srd_Latn</th>\n",
       "      <td>sardinian</td>\n",
       "      <td>srd_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vec_Latn</th>\n",
       "      <td>venetian</td>\n",
       "      <td>vec_Latn</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>azb_Arab</th>\n",
       "      <td>south</td>\n",
       "      <td>azb_Arab</td>\n",
       "      <td>azarbaijani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>azj_Latn</th>\n",
       "      <td>north</td>\n",
       "      <td>azj_Latn</td>\n",
       "      <td>azarbaijani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tur_Latn</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>tur_Latn</td>\n",
       "      <td>azarbaijani</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lang      code    langgroup\n",
       "lmo_Latn    lombard  lmo_Latn      italian\n",
       "eng_Latn    English  eng_Latn      English\n",
       "ita_Latn    italian  ita_Latn      italian\n",
       "fur_Latn   friulian  fur_Latn      italian\n",
       "scn_Latn   sicilian  scn_Latn      italian\n",
       "srd_Latn  sardinian  srd_Latn      italian\n",
       "vec_Latn   venetian  vec_Latn      italian\n",
       "azb_Arab      south  azb_Arab  azarbaijani\n",
       "azj_Latn      north  azj_Latn  azarbaijani\n",
       "tur_Latn    Turkish  tur_Latn  azarbaijani"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lang=pd.DataFrame(metadata).T\n",
    "all_lang.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "798883f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eval_langs=[\"eng_Latn\",\"lmo_Latn\",\"ita_Latn\",\"fur_Latn\",\"scn_Latn\",\"srd_Latn\",\"vec_Latn\",\"azb_Arab\",\"azj_Latn\",\"tur_Latn\",\"kmr_Latn\",\"ckb_Arab\",\"nno_Latn\",\"nob_Latn\",\"lim_Latn\",\"ltz_Latn\",\"nld_Latn\",\"lvs_Latn\",\"ltg_Latn\",\"acm_Arab\",\"acq_Arab\",\"aeb_Arab\",\"ajp_Arab\",\"apc_Arab\",\"arb_Arab\",\"ars_Arab\",\"ary_Arab\",\"arz_Arab\",\"kab_Latn\",\"asm_Beng\",\"ben_Beng\",\"lij_Latn\",\"oci_Latn\",\"yue_Hant\",\"zho_Hans\",\"zho_Hant\",\"glg_Latn\",\"spa_Latn\",\"por_Latn\",\"nso_Latn\",\"sot_Latn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "feb985f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eng_Latn', 'lmo_Latn', 'ita_Latn', 'fur_Latn', 'scn_Latn', 'srd_Latn', 'vec_Latn', 'azb_Arab', 'azj_Latn', 'tur_Latn', 'kmr_Latn', 'ckb_Arab', 'nno_Latn', 'nob_Latn', 'lim_Latn', 'ltz_Latn', 'nld_Latn', 'lvs_Latn', 'ltg_Latn', 'acm_Arab', 'acq_Arab', 'aeb_Arab', 'ajp_Arab', 'apc_Arab', 'arb_Arab', 'ars_Arab', 'ary_Arab', 'arz_Arab', 'kab_Latn', 'asm_Beng', 'ben_Beng', 'lij_Latn', 'oci_Latn', 'yue_Hant', 'zho_Hans', 'zho_Hant', 'glg_Latn', 'spa_Latn', 'por_Latn', 'nso_Latn', 'sot_Latn']\n"
     ]
    }
   ],
   "source": [
    "print(all_eval_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71c23151",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lang='eng_Latn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20c54dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset dialect_nli (.cache/dialect_nli/eng_Latn/1.1.0/b69815628a902151a9f2b158e6be8fabf359868aa4a25c29c09ff689455041b9)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 363.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 5010\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 2490\n",
      "    })\n",
      "})\n",
      "{'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang=train_lang\n",
    "dataset = load_dataset(\"scripts/nli/dialect_nli.py\", lang,\n",
    "            cache_dir=CACHE_DIR)\n",
    "print(dataset)\n",
    "print(dataset['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "079613c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset dialect_nli (.cache/dialect_nli/lmo_Latn/1.1.0/b69815628a902151a9f2b158e6be8fabf359868aa4a25c29c09ff689455041b9)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 963.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 0\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 5010\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 0\n",
      "    })\n",
      "})\n",
      "{'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang='lmo_Latn'\n",
    "dataset = load_dataset(\"scripts/nli/dialect_nli.py\", lang,\n",
    "            cache_dir=CACHE_DIR)\n",
    "print(dataset)\n",
    "print(dataset['test'].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461fc3d",
   "metadata": {},
   "source": [
    "### 10. Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b719e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnv-adp-l",
   "language": "python",
   "name": "vnv-adp-l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
