{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/gscratch/argon/kahuja/.cache/\"\n",
    "LANG = \"aeb_Arab\"\n",
    "DATA_DIR = \"../data/sentiment_analysis/arabic/\"\n",
    "MODEL = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35326c82c8f48df8841d51e21c1bcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 12303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d31faa8dbf4942803b980074b21102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3403 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 3402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f3e32911914913a992087b4081c626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation size: 1368\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "for split in [\"train\", \"test\", \"validation\"]:\n",
    "    datasets[split] = pd.read_csv(os.path.join(DATA_DIR, LANG, f\"{split}.csv\"), header = None)\n",
    "    datasets[split].columns = [\"text\", \"label\"]\n",
    "    datasets[split] = Dataset.from_pandas(datasets[split])\n",
    "    datasets[split] = datasets[split].filter(lambda example: not( example[\"text\"] == \"sentence\" and example[\"label\"] == \"label\"))\n",
    "    print(f\"{split} size: {len(datasets[split])}\")\n",
    "    \n",
    "datasets = DatasetDict(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative', 'positive'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(datasets[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12303\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3402\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1368\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 12303\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_examples(dataset, fs_per_label = 4, seed = 42):\n",
    "    random.seed(seed)\n",
    "    labels = list(set(dataset[\"label\"]))\n",
    "    few_shot_examples = []\n",
    "    for label in labels:\n",
    "        label_examples = dataset.filter(lambda example: example[\"label\"] == label)\n",
    "        # shuffle the examples\n",
    "        label_examples = label_examples.shuffle(seed = seed)\n",
    "        # get the first fs_per_label examples\n",
    "        label_examples = label_examples.select(range(min(fs_per_label, len(label_examples))))\n",
    "        few_shot_examples += [example for example in label_examples]\n",
    "    \n",
    "    #Shuffle the few shot examples\n",
    "    random.shuffle(few_shot_examples)\n",
    "    return few_shot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619bfd2dd900465cab893d9061a26429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7671fb3261fc46fdaac11b0bc3208486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of few shot examples: 8\n"
     ]
    }
   ],
   "source": [
    "few_shot_examples = get_few_shot_examples(datasets[\"train\"], fs_per_label = 4, seed = 42)\n",
    "print(f\"Number of few shot examples: {len(few_shot_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'يا سيدي على عيشوشة والله تمثل مليح وتمثيلها مقنع',\n",
       "  'label': 'positive'},\n",
       " {'text': 'fibelou dhamer ema mehwech motrobi. yetkelem belghacha. wallah meye7chemch',\n",
       "  'label': 'negative'},\n",
       " {'text': 'sl3a pffffffffffffffffffffff', 'label': 'negative'},\n",
       " {'text': 'ملًا رويق', 'label': 'negative'},\n",
       " {'text': 'أحسن فنان', 'label': 'positive'},\n",
       " {'text': 'Maaasta', 'label': 'negative'},\n",
       " {'text': 'Chou ma7lek wenti mastoura ya5tyyyyy', 'label': 'positive'},\n",
       " {'text': 'Ja3four ma7lek oi ma7la idmarik inti il im7ali italvsa oi ena n7bk barcha oi  heoi mouch 5ayeb oi i7eb i3anet ines bravo',\n",
       "  'label': 'positive'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(test_example, fs_examples, prompt_for_each_label = False, labels = []):\n",
    "    \n",
    "    def example_to_prompt(example, add_label=True):\n",
    "        ex_prompt = f\"Sentence: {example['text']}\\n\"\n",
    "        if add_label:\n",
    "            ex_prompt += f\"Label: {example['label']}\\n\"\n",
    "        return ex_prompt\n",
    "        \n",
    "    # To Do: Add domain of the text in the instruction like \"In this task you given text from {domain}\"\n",
    "    prompt = \"In this task, you are given a piece of text. Your task is to classify the sentiment of the text based on its content.\\n\"\n",
    "    \n",
    "    for example in fs_examples:\n",
    "        prompt += example_to_prompt(example, add_label = True)\n",
    "        prompt += \"\\n\"\n",
    "    \n",
    "    if not prompt_for_each_label:\n",
    "        prompt += example_to_prompt(test_example, add_label = False)\n",
    "        return prompt\n",
    "    else:\n",
    "        prompts = [\n",
    "            prompt + example_to_prompt(test_example, add_label = True)\n",
    "            for label in labels\n",
    "        ]\n",
    "        gold_label_idx = labels.index(test_example[\"label\"])\n",
    "        return prompts, gold_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this task, you are given a piece of text. Your task is to classify the sentiment of the text based on its content.\n",
      "Sentence: يا سيدي على عيشوشة والله تمثل مليح وتمثيلها مقنع\n",
      "Label: positive\n",
      "\n",
      "Sentence: fibelou dhamer ema mehwech motrobi. yetkelem belghacha. wallah meye7chemch\n",
      "Label: negative\n",
      "\n",
      "Sentence: sl3a pffffffffffffffffffffff\n",
      "Label: negative\n",
      "\n",
      "Sentence: ملًا رويق\n",
      "Label: negative\n",
      "\n",
      "Sentence: أحسن فنان\n",
      "Label: positive\n",
      "\n",
      "Sentence: Maaasta\n",
      "Label: negative\n",
      "\n",
      "Sentence: Chou ma7lek wenti mastoura ya5tyyyyy\n",
      "Label: positive\n",
      "\n",
      "Sentence: Ja3four ma7lek oi ma7la idmarik inti il im7ali italvsa oi ena n7bk barcha oi  heoi mouch 5ayeb oi i7eb i3anet ines bravo\n",
      "Label: positive\n",
      "\n",
      "Sentence: ليلى متمكنة برشا من المسرح\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(construct_prompt(\n",
    "    test_example = datasets[\"test\"][0],\n",
    "    fs_examples = few_shot_examples,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['In this task, you are given a piece of text. Your task is to classify the sentiment of the text based on its content.\\nSentence: يا سيدي على عيشوشة والله تمثل مليح وتمثيلها مقنع\\nLabel: positive\\n\\nSentence: fibelou dhamer ema mehwech motrobi. yetkelem belghacha. wallah meye7chemch\\nLabel: negative\\n\\nSentence: sl3a pffffffffffffffffffffff\\nLabel: negative\\n\\nSentence: ملًا رويق\\nLabel: negative\\n\\nSentence: أحسن فنان\\nLabel: positive\\n\\nSentence: Maaasta\\nLabel: negative\\n\\nSentence: Chou ma7lek wenti mastoura ya5tyyyyy\\nLabel: positive\\n\\nSentence: Ja3four ma7lek oi ma7la idmarik inti il im7ali italvsa oi ena n7bk barcha oi  heoi mouch 5ayeb oi i7eb i3anet ines bravo\\nLabel: positive\\n\\nSentence: ليلى متمكنة برشا من المسرح\\nLabel: positive\\n', 'In this task, you are given a piece of text. Your task is to classify the sentiment of the text based on its content.\\nSentence: يا سيدي على عيشوشة والله تمثل مليح وتمثيلها مقنع\\nLabel: positive\\n\\nSentence: fibelou dhamer ema mehwech motrobi. yetkelem belghacha. wallah meye7chemch\\nLabel: negative\\n\\nSentence: sl3a pffffffffffffffffffffff\\nLabel: negative\\n\\nSentence: ملًا رويق\\nLabel: negative\\n\\nSentence: أحسن فنان\\nLabel: positive\\n\\nSentence: Maaasta\\nLabel: negative\\n\\nSentence: Chou ma7lek wenti mastoura ya5tyyyyy\\nLabel: positive\\n\\nSentence: Ja3four ma7lek oi ma7la idmarik inti il im7ali italvsa oi ena n7bk barcha oi  heoi mouch 5ayeb oi i7eb i3anet ines bravo\\nLabel: positive\\n\\nSentence: ليلى متمكنة برشا من المسرح\\nLabel: positive\\n'], 0)\n"
     ]
    }
   ],
   "source": [
    "print(construct_prompt(\n",
    "    test_example = datasets[\"test\"][0],\n",
    "    fs_examples = few_shot_examples,\n",
    "    prompt_for_each_label = True,\n",
    "    labels = list(set(datasets[\"train\"][\"label\"]))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa547c3c7a2a44fba6927f65711fb894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL, cache_dir=CACHE_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, cache_dir=CACHE_DIR)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_neg_log_prob(model, tokenizer, prompt, device):\n",
    "    \n",
    "    model.eval()\n",
    "    tokenized_out = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized_out[\"input_ids\"].to(device)\n",
    "    labels = input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, labels=labels)\n",
    "    \n",
    "    return output.loss\n",
    "\n",
    "def generate(model, tokenizer, prompt, device, max_tokens = 20):\n",
    "    \n",
    "    tokenized_out = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized_out[\"input_ids\"].to(device)\n",
    "    labels = tokenized_out[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_new_tokens=max_tokens)\n",
    "    generated_text= tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    prefix_to_remove = prompt\n",
    "    if generated_text.startswith(prefix_to_remove):\n",
    "        generated_text = generated_text[len(prefix_to_remove):].strip()\n",
    "#     generated_text = generated_text.split(\"\\n\\n\")[0].split(\"Label:\")[-1].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "    \n",
    "def evaluate_model_nll(model, tokenizer, test_prompts):\n",
    "    preds = []\n",
    "    acc = 0\n",
    "    for test_prompt in tqdm(test_prompts):\n",
    "        neg_log_probA = get_neg_log_prob(model, tokenizer, test_prompt[\"correct_prompt\"])\n",
    "        neg_log_probB = get_neg_log_prob(model, tokenizer, test_prompt[\"incorrect_prompt\"])\n",
    "        if neg_log_probA < neg_log_probB:\n",
    "            acc += 1\n",
    "    acc = acc / len(test_prompts)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def process_text(text):\n",
    "#     return text.split(\"Label:\")[-1].strip().lower()\n",
    "    return text.lower()\n",
    "\n",
    "def process_generation(generation):\n",
    "    return generation.split(\"\\n\\n\")[0].split(\"Label:\")[-1].strip().lower()\n",
    "\n",
    "def evaluate_generation(generated_text, label):\n",
    "    return float(process_generation(generated_text) == process_text(label))\n",
    "\n",
    "def evaluate_model_gen(model, tokenizer, test_prompts, device):\n",
    "    \n",
    "    preds = []\n",
    "    correct_or_not = []\n",
    "    accs = 0\n",
    "    for test_prompt in tqdm(test_prompts):\n",
    "        generated_text = generate(model, tokenizer, test_prompt[\"prompt\"], device = device)\n",
    "        preds.append(process_text(generated_text))\n",
    "        correct_or_not.append(evaluate_generation(generated_text, test_prompt[\"label\"]))\n",
    "    \n",
    "    return np.mean(correct_or_not), preds, correct_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [{\"prompt\": construct_prompt(test_example, few_shot_examples), \"label\": test_example[\"label\"]}\n",
    "                    for _, test_example in zip(range(5), datasets[\"validation\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf4b390a8b14c93a3d3586431a7a1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "acc, preds, correct_or_not = evaluate_model_gen(model,\n",
    "                                                tokenizer, \n",
    "                                                test_prompts, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
